# sentinel_app/app.py

from pathlib import Path
import joblib
import numpy as np
import pandas as pd
import streamlit as st

# ---------- Paths ----------
APP_DIR = Path(__file__).resolve().parent          # .../sentinel_app
PROJECT_ROOT = APP_DIR.parent                      # .../sentinel
MODELS_DIR = PROJECT_ROOT / "models"
DATA_PATH = PROJECT_ROOT / "data" / "processed" / "tradyflow_training.parquet"

# ---------- Streamlit setup ----------

st.set_page_config(
    page_title="Sentinel â€“ Baseline ML Panel",
    layout="wide",
)

st.title("ðŸ”® Sentinel â€“ Baseline Modeling Panel")
st.caption(
    "Baseline ML models trained on options sweep features. "
    "This shell wires notebooks â†’ models â†’ interactive dashboard."
)

# ---------- Load artifacts ----------


@st.cache_resource
def load_models():
    """Load trained models from the models/ directory."""
    direction_rf = joblib.load(MODELS_DIR / "sentinel_direction_up_rf.pkl")
    volregime_rf = joblib.load(MODELS_DIR / "sentinel_vol_regime_rf.pkl")
    nextret_rf = joblib.load(MODELS_DIR / "sentinel_next_return_rf.pkl")

    return {
        "direction_rf": direction_rf,
        "volregime_rf": volregime_rf,
        "nextret_rf": nextret_rf,
    }


@st.cache_data
def load_training_data():
    """Load the modeling dataset with features + targets."""
    df = pd.read_parquet(DATA_PATH)
    return df


def get_feature_cols(df: pd.DataFrame) -> list[str]:
    """
    Recreate the feature column selection from Notebook 04.

    IMPORTANT:
    This MUST match the logic used when training the models
    (same columns, same order). If it ever drifts, copy the exact
    block from 04_model_training.ipynb.
    """
    # Columns we do NOT want as features
    exclude_cols = {
        "Time",
        "Sym",
        "C/P",
        "Exp",
        "next_spot",
        "next_return_1d",
        "direction_up",
        "vol_regime",
    }

    numeric_cols = df.select_dtypes(include=["float64", "int64"]).columns
    feature_cols = [c for c in numeric_cols if c not in exclude_cols]

    return feature_cols


def get_top_features(model, feature_cols, k: int = 5):
    """Return top-k (feature, importance) pairs for a fitted tree-based model."""
    importances = model.feature_importances_
    pairs = sorted(
        zip(feature_cols, importances),
        key=lambda x: x[1],
        reverse=True,
    )[:k]
    return pairs


# ---------- Load everything ----------

models = load_models()
st.success("Models loaded from `models/`")

df = load_training_data()
feature_cols = get_feature_cols(df)

st.write(
    f"Training dataset loaded with **{len(df)}** rows and **{len(feature_cols)}** features."
)

# ---------- Sidebar: pick a sample sweep ----------

with st.sidebar:
    st.header("Sample Sweep Selector")
    st.caption("Use a real row from the training set to sanity-check model wiring.")

    max_idx = len(df) - 1
    idx = st.number_input(
        "Row index",
        min_value=0,
        max_value=max_idx,
        value=0,
        step=1,
    )

    sample = df.iloc[[idx]]  # keep as DataFrame

# ---------- Show input features ----------

st.subheader("1. Input Features (Engineered Row)")
st.dataframe(sample[feature_cols], use_container_width=True)

# ---------- Run predictions ----------

X_sample = sample[feature_cols]

# Direction: probability next move is up
dir_proba_up = models["direction_rf"].predict_proba(X_sample)[0, 1]
dir_label = "â¬†ï¸ Up" if dir_proba_up >= 0.5 else "â¬‡ï¸ Down / Flat"

# Vol regime: 0 = normal, 1 = high vol
vol_pred = models["volregime_rf"].predict(X_sample)[0]
vol_proba = models["volregime_rf"].predict_proba(X_sample)[0, int(vol_pred)]
vol_label = "ðŸŒª High volatility" if vol_pred == 1 else "ðŸŒ¤ Normal volatility"

# Next 1D return prediction
next_ret_pred = models["nextret_rf"].predict(X_sample)[0]

# ---------- Headline metrics ----------

st.subheader("2. Headline Predictions")

col1, col2, col3 = st.columns(3)

with col1:
    st.metric(
        label="P(Direction Up)",
        value=f"{dir_proba_up:.1%}",
        delta=dir_label,
    )

with col2:
    st.metric(
        label="Volatility Regime",
        value=vol_label,
        delta=f"Confidence {vol_proba:.1%}",
    )

with col3:
    st.metric(
        label="Predicted Next 1D Return",
        value=f"{next_ret_pred:.3f}",
    )

# ---------- Quick interpretation ----------

st.subheader("3. Quick Model Interpretation")

# Global top features (from training) for context
top_dir = get_top_features(models["direction_rf"], feature_cols, k=3)
top_vol = get_top_features(models["volregime_rf"], feature_cols, k=3)
top_ret = get_top_features(models["nextret_rf"], feature_cols, k=3)

dir_feat_names = ", ".join(f for f, _ in top_dir)
vol_feat_names = ", ".join(f for f, _ in top_vol)
ret_feat_names = ", ".join(f for f, _ in top_ret)

# Simple sign-based interpretation of the predicted return
if next_ret_pred > 0.01:
    ret_view = "slightly bullish"
elif next_ret_pred < -0.01:
    ret_view = "slightly bearish"
else:
    ret_view = "near-flat / noisy"

st.markdown(
    f"""
- The **direction model** assigns **{dir_proba_up:.1%}** probability that the next move is up.
  It leans most on features like **{dir_feat_names}**.
- The **volatility model** classifies this environment as **{vol_label}** with **{vol_proba:.1%}** confidence,
  driven by signals such as **{vol_feat_names}**.
- The **return model** expects a next 1D return of **{next_ret_pred:.3f}**, which we interpret as **{ret_view}**.
  Its strongest drivers include **{ret_feat_names}**.
"""
)

# ---------- Notes section ----------

st.subheader("4. What this shell proves")

st.markdown(
    """
- âœ… Models load correctly from disk.
- âœ… `feature_cols` selection is consistent with Notebook 04.
- âœ… We can pass a **real engineered row** through all three models:
  - Direction (up / down)
  - Volatility regime (normal vs high)
  - Short-horizon return magnitude
- âœ… This structure is ready for:
  - A user-friendly input form for new sweeps
  - Feature importance charts and confusion matrices
  - Integration into Azure (App Service / Container Apps)
"""
)

# ---------- Deeper dive: feature importance overview ----------

st.subheader("5. Feature Importance Overview")

with st.expander("Show global feature importances", expanded=False):
    # Direction RF
    st.markdown("**Direction model (Random Forest)**")
    dir_imp = (
        pd.DataFrame(
            {"feature": feature_cols,
             "importance": models["direction_rf"].feature_importances_}
        )
        .sort_values("importance", ascending=False)
        .head(15)
        .set_index("feature")
    )
    st.bar_chart(dir_imp)

    # Volatility regime RF
    st.markdown("**Volatility regime model (Random Forest)**")
    vol_imp = (
        pd.DataFrame(
            {"feature": feature_cols,
             "importance": models["volregime_rf"].feature_importances_}
        )
        .sort_values("importance", ascending=False)
        .head(15)
        .set_index("feature")
    )
    st.bar_chart(vol_imp)

    # Return RF (regressor)
    st.markdown("**Return model (Random Forest Regressor)**")
    ret_imp = (
        pd.DataFrame(
            {"feature": feature_cols,
             "importance": models["nextret_rf"].feature_importances_}
        )
        .sort_values("importance", ascending=False)
        .head(15)
        .set_index("feature")
    )
    st.bar_chart(ret_imp)

    st.caption(
        "These charts confirm that the models rely on intuitive signals such as "
        "moneyness, spreads, and time-to-expiry, rather than random noise."
    )
